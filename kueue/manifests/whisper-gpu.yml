# whisper-job-cpu.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: whisper-transcription
  namespace: sai # Change this to your target namespace
  labels:
    kueue.x-k8s.io/queue-name: "local-queue"
spec:
  parallelism: 2
  completions: 6
  completionMode: Indexed
  template:
    spec:
      # Use an initContainer to download the audio file based on JOB_COMPLETION_INDEX
      initContainers:
      - name: download-audio
        image: quay.io/smalleni/alpine:latest
        command: ["/bin/sh", "-c"]
        args:
        - |
          echo "Running download script..."
          /scripts/download-audio.sh
        env:
        - name: GITHUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: github-token
              key: token
        volumeMounts:
        - name: audio-data
          mountPath: /data
        - name: download-script
          mountPath: /scripts

      # Main container to run the transcription
      containers:
      - name: whisper-transcriber
        image: quay.io/smalleni/whisper:latest
        # Command to run whisper on the downloaded file
        command: ["/bin/sh", "-c"]
        args:
        - |
          FILE=$(ls /data/* | head -n 1)
          whisper "$FILE" --model_dir /tmp/whisper_models --model tiny.en --output_dir /tmp
        resources:
          requests:
            cpu: "2"       # Request 2 CPU cores
            memory: "4Gi"  # Request 4 GiB of memory
          #limits:
          #  nvidia.com/gpu: 1
        volumeMounts:
        - name: audio-data
          mountPath: /data
        - name: model-cache-volume
          mountPath: /tmp/whisper_models
      volumes:
      - name: audio-data
        emptyDir: {} # An ephemeral volume to share data between containers
      - name: model-cache-volume
        emptyDir: {}
      - name: download-script
        configMap:
          name: audio-download-script
          defaultMode: 0755 # Make the script executable
      restartPolicy: Never
  backoffLimit: 2
